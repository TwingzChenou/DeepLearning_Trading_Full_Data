{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888fc958-f76c-4967-9ce8-fd720620c888",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importation Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf644b15-60d5-4f65-a328-b012efa41117",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "import yfinance as yf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import plotly.express as px\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras_tuner import RandomSearch\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import  mixed_precision, layers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"CPU devices:\", tf.config.list_physical_devices('CPU'))\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Mixed precision (gain VRAM + vitesse si GPU récent)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Optional: limiter verbosité TF\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ],
   "id": "70fa4a4e3d240f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_data = pd.DataFrame()\n",
    "data_SP500 = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_SP500.parquet')\n",
    "data_NDX = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_NASDAQ.parquet')\n",
    "data_MP = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_MP.parquet')\n",
    "data_Crypto = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_Crypto.parquet')\n",
    "daily_data = pd.concat([data_SP500, data_NDX, data_MP, data_Crypto], ignore_index=True)\n",
    "daily_data = daily_data.drop_duplicates(subset=['Ticker', 'Date'])\n",
    "daily_data"
   ],
   "id": "d2e4614d0e961b9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9261fbb-ea78-4ee0-9018-8a2a48f48f1a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#start_date = \"1990-01-03\"\n",
    "#final_date = \"2025-07-16\"\n",
    "\n",
    "daily_data = daily_data.sort_values(by=['Date'], ascending = True)\n",
    "\n",
    "# Accéder à l'index de la colonne 'Date'\n",
    "#s = daily_data[daily_data['Date'] == start_date].index[0]\n",
    "#f = daily_data[daily_data['Date'] == final_date].index[0]\n",
    "\n",
    "# Filtrer les données à partir de la date spécifiée\n",
    "#daily_data = daily_data.loc[s:]\n",
    "\n",
    "daily_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "109b788b-7ff7-4eed-9082-8714009e6cd6",
   "metadata": {},
   "source": [
    "#Date au format Date\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "# Définir la colonne 'Date' comme index\n",
    "daily_data.set_index('Date', inplace=True)\n",
    "\n",
    "daily_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "660ccfad-e379-4ebe-b511-bcb779337cda",
   "metadata": {},
   "source": [
    "\n",
    "# Rééchantillonner les données en fréquence Annuelle\n",
    "yearly_data = daily_data.groupby(\"Ticker\").resample('YE').agg({\n",
    "    'Open': 'first',   # Premier prix d'ouverture du mois\n",
    "    'High': 'max',     # Plus haut du mois\n",
    "    'Low': 'min',      # Plus bas du mois\n",
    "    'Close': 'last',   # Dernier prix de clôture du mois\n",
    "    'Volume': 'sum',   # Somme du volume sur le mois\n",
    "}).reset_index()\n",
    "yearly_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7da764e7-0e05-4c8d-977c-b5f895fecf79",
   "metadata": {},
   "source": [
    "\n",
    "# Rééchantillonner les données en fréquence mensuelle\n",
    "monthly_data = daily_data.groupby(\"Ticker\").resample('ME').agg({\n",
    "    'Open': 'first',   # Premier prix d'ouverture du mois\n",
    "    'High': 'max',     # Plus haut du mois\n",
    "    'Low': 'min',      # Plus bas du mois\n",
    "    'Close': 'last',   # Dernier prix de clôture du mois\n",
    "    'Volume': 'sum',   # Somme du volume sur le mois\n",
    "}).reset_index()\n",
    "monthly_data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b35eba8d-a88b-4014-8f50-d8042c822620",
   "metadata": {},
   "source": [
    "\n",
    "# Rééchantillonner les données en fréquence semestrielle\n",
    "weekly_data = daily_data.groupby(\"Ticker\").resample('W').agg({\n",
    "    'Open': 'first',   # Premier prix d'ouverture du mois\n",
    "    'High': 'max',     # Plus haut du mois\n",
    "    'Low': 'min',      # Plus bas du mois\n",
    "    'Close': 'last',   # Dernier prix de clôture du mois\n",
    "    'Volume': 'sum',   # Somme du volume sur le mois\n",
    "}).reset_index()\n",
    "weekly_data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2536e299-2561-40db-adaa-f56455d8d35f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Labelling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "daily_data = daily_data.reset_index()",
   "id": "a3d8e36e0e568cce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Target Pourcentage",
   "id": "10c100ec271e8e5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Best_And_Worst_Stocks_Pourcentage(snp_prices, pourcentage, window):\n",
    "    # Copie pour ne pas altérer les données d'origine\n",
    "    snp_prices_copy = snp_prices.copy()\n",
    "\n",
    "    # Calcul de la performance glissante par ticker\n",
    "    snp_prices_copy['Performance'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "    # Supprimer les lignes avec NaN (ex: début de série)\n",
    "    snp_prices_copy = snp_prices_copy.dropna(subset=['Performance'])\n",
    "\n",
    "    # Ajouter l'année cible (année suivante car performance à horizon futur)\n",
    "    snp_prices_copy['Year'] = snp_prices_copy['Date'].dt.year\n",
    "    snp_prices_copy['Month'] = snp_prices_copy['Date'].dt.month\n",
    "\n",
    "    top_stocks = snp_prices_copy[snp_prices_copy['Performance'] > pourcentage]\n",
    "    flop_stocks = snp_prices_copy[snp_prices_copy['Performance'] < pourcentage]\n",
    "\n",
    "    # Sélection finale\n",
    "    top_stocks = top_stocks[['Month','Year', 'Ticker', 'Performance']]\n",
    "    print(top_stocks)\n",
    "    flop_stocks = flop_stocks[['Month', 'Year', 'Ticker', 'Performance']]\n",
    "\n",
    "    return top_stocks, flop_stocks"
   ],
   "id": "a9cbc1b19b7fead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Merge_Performance_Pourcentage(snp, data, window, pourcentage):\n",
    "    # Obtenir les top et flop stocks\n",
    "    top_stocks, flop_stocks = Best_And_Worst_Stocks_Pourcentage(data, pourcentage, window)\n",
    "\n",
    "    # Ajouter la colonne Year pour joindre\n",
    "    snp['Date'] = pd.to_datetime(snp['Date'])\n",
    "    snp['Year'] = snp['Date'].dt.year\n",
    "    snp['Month'] = snp['Date'].dt.month\n",
    "    #snp['Year'] = snp.apply(lambda row: row['Year'] + 1 if row['Date'].month == 12 else row['Year'], axis=1)\n",
    "\n",
    "    # Fusionner avec top et flop\n",
    "    snp = snp.sort_values(by=['Ticker', 'Year', 'Month'])\n",
    "    top_stocks = top_stocks.sort_values(by=['Ticker', 'Year', 'Month'])\n",
    "    flop_stocks = flop_stocks.sort_values(by=['Ticker', 'Year', 'Month'])\n",
    "\n",
    "    # Marquage top\n",
    "    snp = pd.merge(snp, top_stocks, on=['Ticker', 'Year', 'Month'], how='left', suffixes=('', '_top'))\n",
    "    snp['Top_Label'] = (snp['Performance'] > 0).astype(int)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    # Marquage flop\n",
    "    snp = pd.merge(snp, flop_stocks, on=['Ticker', 'Year', 'Month'], how='left', suffixes=('', '_flop'))\n",
    "    snp['Flop_Label'] = (snp['Performance'] < 0).astype(int)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    return snp"
   ],
   "id": "f12c76f1fd5aa521",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Top Momentum ajusted Vol",
   "id": "f304c18238af3d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Best_And_Worst_Stocks_Momentum_Ajusted_Vol_Years(snp_prices, top_n, window):\n",
    "    # Copie pour ne pas altérer les données d'origine\n",
    "    snp_prices_copy = snp_prices.copy()\n",
    "\n",
    "    #Calcul du rendement par année glissante\n",
    "    snp_prices_copy['Return'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change()\n",
    "\n",
    "    # Calcul de la performance glissante par ticker\n",
    "    snp_prices_copy['Performance'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "    # Calcul de la volatilité glissante\n",
    "    snp_prices_copy[\"Vol\"] = snp_prices_copy.groupby('Ticker')['Return'].rolling(window).std().reset_index(level=0, drop=True)\n",
    "    # Calcul du Momentum Vol Ajusted\n",
    "    snp_prices_copy['Momentum Ajusted Vol'] = snp_prices_copy['Performance'] / snp_prices_copy['Vol']\n",
    "    print(snp_prices_copy)\n",
    "\n",
    "    # Replace infinite updated data with nan\n",
    "    snp_prices_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Supprimer les lignes avec NaN (ex: début de série)\n",
    "    snp_prices_copy = snp_prices_copy.dropna(subset=['Momentum Ajusted Vol'])\n",
    "\n",
    "\n",
    "    # Ajouter l'année cible (année suivante car performance à horizon futur)\n",
    "    snp_prices_copy['Year'] = snp_prices_copy['Date'].dt.year\n",
    "    #snp_prices_copy['Month'] = snp_prices_copy['Date'].dt.month\n",
    "\n",
    "    # Trier par Date et Performance descendante pour top, ascendante pour flop\n",
    "    sorted_df = snp_prices_copy.sort_values(by=['Date', 'Momentum Ajusted Vol'], ascending=[True, False])\n",
    "    top_stocks = sorted_df.groupby('Date').head(top_n)\n",
    "\n",
    "    sorted_df_flop = snp_prices_copy.sort_values(by=['Date', 'Momentum Ajusted Vol'], ascending=[True, True])\n",
    "    flop_stocks = sorted_df_flop.groupby('Date').head(top_n)\n",
    "\n",
    "    # Sélection finale\n",
    "    top_stocks = top_stocks[['Year', 'Ticker', 'Momentum Ajusted Vol']]\n",
    "    flop_stocks = flop_stocks[['Year', 'Ticker', 'Momentum Ajusted Vol']]\n",
    "\n",
    "    return top_stocks, flop_stocks"
   ],
   "id": "bd174b0cd43baf21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Merge_Performance_Years_Top_Momentum_Ajusted_Vol(snp, data, window, top_n):\n",
    "    # Obtenir les top et flop stocks\n",
    "    top_stocks, flop_stocks = Best_And_Worst_Stocks_Momentum_Ajusted_Vol_Years(data, top_n, window)\n",
    "\n",
    "    # Ajouter la colonne Year pour joindre\n",
    "    snp['Date'] = pd.to_datetime(snp['Date'])\n",
    "    snp['Year'] = snp['Date'].dt.year\n",
    "    #snp['Month'] = snp['Date'].dt.month\n",
    "    #snp['Year'] = snp.apply(lambda row: row['Year'] + 1 if row['Date'].month == 12 else row['Year'], axis=1)\n",
    "\n",
    "    # Fusionner avec top et flop\n",
    "    snp = snp.sort_values(by=['Ticker', 'Year'])\n",
    "    top_stocks = top_stocks.sort_values(by=['Ticker', 'Year'])\n",
    "    flop_stocks = flop_stocks.sort_values(by=['Ticker', 'Year'])\n",
    "\n",
    "    # Marquage top\n",
    "    snp = pd.merge(snp, top_stocks, on=['Ticker', 'Year'], how='left', suffixes=('', '_top'))\n",
    "    snp['Top_Label'] = (snp['Momentum Ajusted Vol'] > 0).astype(int)\n",
    "    snp = snp.drop(columns=['Momentum Ajusted Vol'])\n",
    "\n",
    "    # Marquage flop\n",
    "    snp = pd.merge(snp, flop_stocks, on=['Ticker', 'Year'], how='left', suffixes=('', '_flop'))\n",
    "    snp['Flop_Label'] = (snp['Momentum Ajusted Vol'] < 0).astype(int)\n",
    "    snp = snp.drop(columns=['Momentum Ajusted Vol'])\n",
    "\n",
    "    return snp"
   ],
   "id": "4829066003f0e0bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d854ec61-2801-4bed-9cbe-1b14f017042e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "### Target Top"
  },
  {
   "cell_type": "code",
   "id": "7768b971-5260-4d06-aad0-896196664b10",
   "metadata": {},
   "source": [
    "def Best_And_Worst_Stocks_Years(snp_prices, top_n, window):\n",
    "    # Copie pour ne pas altérer les données d'origine\n",
    "    snp_prices_copy = snp_prices.copy()\n",
    "\n",
    "    # Calcul de la performance glissante par ticker\n",
    "    snp_prices_copy['Performance'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "    # Supprimer les lignes avec NaN (ex: début de série)\n",
    "    snp_prices_copy = snp_prices_copy.dropna(subset=['Performance'])\n",
    "\n",
    "    # Ajouter l'année cible (année suivante car performance à horizon futur)\n",
    "    snp_prices_copy['Year'] = snp_prices_copy['Date'].dt.year\n",
    "    #snp_prices_copy['Month'] = snp_prices_copy['Date'].dt.month\n",
    "\n",
    "    # Trier par Date et Performance descendante pour top, ascendante pour flop\n",
    "    sorted_df = snp_prices_copy.sort_values(by=['Date', 'Performance'], ascending=[True, False])\n",
    "    top_stocks = sorted_df.groupby('Date').head(top_n)\n",
    "\n",
    "    sorted_df_flop = snp_prices_copy.sort_values(by=['Date', 'Performance'], ascending=[True, True])\n",
    "    flop_stocks = sorted_df_flop.groupby('Date').head(top_n)\n",
    "\n",
    "    # Sélection finale\n",
    "    top_stocks = top_stocks[['Year', 'Ticker', 'Performance']]\n",
    "    flop_stocks = flop_stocks[['Year', 'Ticker', 'Performance']]\n",
    "    print(top_stocks)\n",
    "\n",
    "    return top_stocks, flop_stocks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c710c63c-2116-4b14-b045-01e578a85f34",
   "metadata": {},
   "source": [
    "def Merge_Performance_Years_Top(snp, data, window, top_n):\n",
    "    # Obtenir les top et flop stocks\n",
    "    top_stocks, flop_stocks = Best_And_Worst_Stocks_Years(data, top_n, window)\n",
    "\n",
    "    # Ajouter la colonne Year pour joindre\n",
    "    snp['Date'] = pd.to_datetime(snp['Date'])\n",
    "    snp['Year'] = snp['Date'].dt.year\n",
    "    #snp['Month'] = snp['Date'].dt.month\n",
    "    #snp['Year'] = snp.apply(lambda row: row['Year'] + 1 if row['Date'].month == 12 else row['Year'], axis=1)\n",
    "\n",
    "    # Fusionner avec top et flop\n",
    "    snp = snp.sort_values(by=['Ticker', 'Year'])\n",
    "    top_stocks = top_stocks.sort_values(by=['Ticker', 'Year'])\n",
    "    flop_stocks = flop_stocks.sort_values(by=['Ticker', 'Year'])\n",
    "\n",
    "    # Marquage top\n",
    "    snp = pd.merge(snp, top_stocks, on=['Ticker', 'Year'], how='left', suffixes=('', '_top'))\n",
    "    snp['Top_Label'] = (snp['Performance'] > 0).astype(int)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    # Marquage flop\n",
    "    snp = pd.merge(snp, flop_stocks, on=['Ticker', 'Year'], how='left', suffixes=('', '_flop'))\n",
    "    snp['Flop_Label'] = (snp['Performance'] < 0).astype(int)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    return snp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Top Rank",
   "id": "84d1108d1450257c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Best_And_Worst_Stocks_Rank(snp, snp_prices, window):\n",
    "    # Copie pour ne pas altérer les données d'origine\n",
    "    snp_prices_copy = snp_prices.copy()\n",
    "\n",
    "    # Calcul de la performance glissante par ticker\n",
    "    snp_prices_copy['Performance'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "    # Supprimer les lignes avec NaN (ex: début de série)\n",
    "    snp_prices_copy = snp_prices_copy.dropna(subset=['Performance'])\n",
    "\n",
    "    # Ajouter l'année cible (année suivante car performance à horizon futur)\n",
    "    snp_prices_copy['Year'] = snp_prices_copy['Date'].dt.year\n",
    "    snp_prices_copy['Month'] = snp_prices_copy['Date'].dt.month\n",
    "\n",
    "    # Trier par Date et Performance descendante pour top, ascendante pour flop\n",
    "    snp_prices_copy[\"Top_Label\"] = snp_prices_copy.groupby(\"Date\")[\"Performance\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    snp_prices_copy[\"Flop_Label\"] = snp_prices_copy.groupby(\"Date\")[\"Performance\"].rank(ascending=True, method=\"first\").astype(int)\n",
    "    top_flop_stocks = snp_prices_copy[['Top_Label', 'Flop_Label', 'Year', 'Month', 'Ticker']]\n",
    "\n",
    "    # Ajouter la colonne Year pour joindre\n",
    "    snp['Date'] = pd.to_datetime(snp['Date'])\n",
    "    snp['Year'] = snp['Date'].dt.year\n",
    "    snp['Month'] = snp['Date'].dt.month\n",
    "\n",
    "    snp = pd.merge(snp, top_flop_stocks, on=['Ticker', 'Year', 'Month'])\n",
    "\n",
    "    return snp"
   ],
   "id": "e3b727578963c31c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Target Daily",
   "id": "ac29df57dbc7c431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Best_And_Worst_Stocks_Daily(snp_prices, top_n, window):\n",
    "    # Copie pour ne pas altérer les données d'origine\n",
    "    snp_prices_copy = snp_prices.copy()\n",
    "\n",
    "    # Calcul de la performance glissante par ticker\n",
    "    snp_prices_copy['Performance'] = snp_prices_copy.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "    # Supprimer les lignes avec NaN (ex: début de série)\n",
    "    snp_prices_copy = snp_prices_copy.dropna(subset=['Performance'])\n",
    "\n",
    "    # Trier par Date et Performance descendante pour top, ascendante pour flop\n",
    "    sorted_df = snp_prices_copy.sort_values(by=['Date', 'Performance'], ascending=[True, False])\n",
    "    top_stocks = sorted_df.groupby('Date').head(top_n)\n",
    "\n",
    "    sorted_df_flop = snp_prices_copy.sort_values(by=['Date', 'Performance'], ascending=[True, True])\n",
    "    flop_stocks = sorted_df_flop.groupby('Date').head(top_n)\n",
    "\n",
    "    # Sélection finale\n",
    "    top_stocks = top_stocks[['Date', 'Ticker', 'Performance']]\n",
    "    flop_stocks = flop_stocks[['Date', 'Ticker', 'Performance']]\n",
    "\n",
    "    return top_stocks, flop_stocks"
   ],
   "id": "4b0c047a50be187b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Merge_Performance_Daily_Top(snp, data, window, top_n):\n",
    "    # Obtenir les top et flop stocks\n",
    "    top_stocks, flop_stocks = Best_And_Worst_Stocks_Daily(data, top_n, window)\n",
    "\n",
    "    # Ajouter la colonne Year pour joindre\n",
    "    snp['Date'] = pd.to_datetime(snp['Date'])\n",
    "\n",
    "    # Fusionner avec top et flop\n",
    "    snp = snp.sort_values(by=['Ticker', 'Date'])\n",
    "    top_stocks = top_stocks.sort_values(by=['Ticker', 'Date'])\n",
    "    flop_stocks = flop_stocks.sort_values(by=['Ticker', 'Date'])\n",
    "\n",
    "    # Marquage top\n",
    "    snp = pd.merge(snp, top_stocks, on=['Ticker', 'Date'], how='left', suffixes=('', '_top'))\n",
    "    snp['Top_Label'] = (snp['Performance'] > 0).astype(int)\n",
    "    snp['Top_Label'] = snp.groupby('Ticker')['Top_Label'].shift(-1)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    # Marquage flop\n",
    "    snp = pd.merge(snp, flop_stocks, on=['Ticker', 'Date'], how='left', suffixes=('', '_flop'))\n",
    "    snp['Flop_Label'] = (snp['Performance'] < 0).astype(int)\n",
    "    snp['Flop_Label'] = snp.groupby('Ticker')['Flop_Label'].shift(-1)\n",
    "    snp = snp.drop(columns=['Performance'])\n",
    "\n",
    "    return snp"
   ],
   "id": "fd49027f12a146fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e51fa94-ec64-4ef9-8a9e-33184386b187",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "id": "048a29cc-56d1-42a7-b187-e8e3027f238a",
   "metadata": {},
   "source": [
    "# Paramètres\n",
    "snp = weekly_data\n",
    "target = yearly_data\n",
    "top_n = 10\n",
    "window = 1\n",
    "label = []\n",
    "pourcentage = 1\n",
    "\n",
    "#data = Merge_Performance_Years_Top_Momentum_Ajusted_Vol(snp, target, window, top_n)\n",
    "data = Merge_Performance_Years_Top(snp, target, window, top_n)\n",
    "#data = Merge_Performance_Pourcentage(snp, target, window, pourcentage)\n",
    "#data = Merge_Performance_Daily_Top(snp, target, window, top_n)\n",
    "\"\"\"\n",
    "data = Best_And_Worst_Stocks_Rank(snp, target, window)\n",
    "data['Label'] = data['Top_Label']\n",
    "\"\"\"\n",
    "data = data.sort_values(by=['Ticker','Date'], ascending=[True, True])\n",
    "for i in data.index:\n",
    "    if data['Top_Label'][i] > 0:\n",
    "        label.append(1)\n",
    "    elif data['Flop_Label'][i] > 0:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(0)\n",
    "data['Label'] = label\n",
    "\n",
    "\n",
    "data = data.drop(columns=['Low', 'High', 'Open', 'Top_Label', 'Flop_Label', 'Year'])\n",
    "data = data.dropna()\n",
    "data['Return'] = data.groupby('Ticker')['Close'].pct_change()\n",
    "data = data.sort_values(by=['Date', 'Label'], ascending=[True, False])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data['Label'].value_counts()",
   "id": "6a4009745b14c78a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.describe()",
   "id": "1be6c58a662fb51b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[data['Label'] == 1].sort_values('Date').tail(20)",
   "id": "756db0346d7a48fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 1) Télécharger les cours mensuels ajustés du S&P 500\n",
    "ndx = yf.download(\n",
    "    \"^GSPC\",\n",
    "    start=\"1976-07-01\",\n",
    "    end=\"2025-07-17\",\n",
    "    interval=\"1wk\",\n",
    "    auto_adjust=True,\n",
    "    actions=False\n",
    ")[[\"Open\", \"Close\"]]\n",
    "\n",
    "ndx.columns = ndx.columns.droplevel(level=1)\n",
    "\n",
    "ndx.columns = ['ndx_Open', 'ndx_Close']\n",
    "\n",
    "# 2) Convertir l’index en période mensuelle, puis au TIMESTAMP de fin de mois\n",
    "ndx.index = pd.to_datetime(ndx.index)\n",
    "\n",
    "# 3) Normaliser pour que le S&P 500 commence à la même valeur que le capital initial\n",
    "ndx['Return'] = (ndx['ndx_Close'].pct_change()).fillna(0)\n",
    "\n",
    "# Calcul des moyennes mobiles pour chaque fenêtre\n",
    "ndx['Slow_ma'] = ndx['ndx_Close'].rolling(window=25, min_periods=1).mean()\n",
    "ndx['Fast_ma'] = ndx['ndx_Close'].rolling(window=9, min_periods=1).mean()\n",
    "\n",
    "crisis = []\n",
    "for i in range(0, len(ndx)):\n",
    "    if ndx['Slow_ma'].iloc[i] < ndx['Fast_ma'].iloc[i] and ndx['Slow_ma'].iloc[i] < ndx['ndx_Close'].iloc[i]:\n",
    "        crisis.append(0)\n",
    "    elif ndx['Slow_ma'].iloc[i] > ndx['Fast_ma'].iloc[i] and ndx['Fast_ma'].iloc[i] < ndx['ndx_Close'].iloc[i]:\n",
    "        crisis.append(0)\n",
    "    else:\n",
    "        crisis.append(1)\n",
    "\n",
    "ndx['Crisis'] = crisis\n",
    "\n",
    "ndx = ndx.fillna(0)\n",
    "ndx = ndx.drop(columns=['ndx_Open', 'ndx_Close','Return', 'Slow_ma', 'Fast_ma'])\n",
    "\n",
    "weekly_data = pd.merge_asof(\n",
    "    weekly_data.sort_values('Date'),\n",
    "    ndx.sort_values('Date'),\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\"\"\"\n",
    "data = data[data['Crisis'] == 0]\n",
    "data = data.drop(columns=['Crisis'])\n",
    "\"\"\""
   ],
   "id": "16fd0f6c1d0da5a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddcff51e-4cdf-4df0-9366-c325a9d00244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "# Features"
  },
  {
   "cell_type": "markdown",
   "id": "7db738c8-25d8-448f-b593-19badbcf6fc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Moyenne Mobile"
   ]
  },
  {
   "cell_type": "code",
   "id": "85190e73-e56d-409b-82c6-e92cbe98f0cc",
   "metadata": {},
   "source": [
    "\n",
    "# Paramètres\n",
    "liste_ma = [9, 25, 50, 100]\n",
    "\n",
    "# Calcul des moyennes mobiles pour chaque fenêtre\n",
    "for window in liste_ma:\n",
    "    # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "    ma_column = f'ma_{window}'\n",
    "    data[ma_column] = data.groupby('Ticker')['Close'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2e1ef9e-0ecf-47a2-b778-eef78e939a6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Distance Moyenne Mobile"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd5e24a0-0cdd-489f-a88a-0eb84e64c0fe",
   "metadata": {},
   "source": [
    "\n",
    "liste_ma_1 = [9, 25, 50, 100]\n",
    "liste_ma_2 = [9, 25, 50, 100]\n",
    "\n",
    "for window in liste_ma_1:\n",
    "    for window_2 in liste_ma_2:\n",
    "        \n",
    "        # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "        ma_column_1 = f'ma_{window}'\n",
    "        ma_column_2 = f'ma_{window_2}'\n",
    "        distance_ma_column = f'distance_ma_{window_2}/{window}'\n",
    "        \n",
    "        if ma_column_1 != ma_column_2:\n",
    "            # Calcul du pourcentage d'écart entre le prix et la moyenne mobile \n",
    "            data[distance_ma_column] = data[ma_column_2] / data[ma_column_1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3f2f214-b0b9-4c81-a3bc-25abd25a959b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Position MA"
   ]
  },
  {
   "cell_type": "code",
   "id": "48a4166a-a059-424f-bbef-39123420a04a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "liste_ma_1 = [9, 25]\n",
    "liste_ma_2 = [9, 25]\n",
    "\n",
    "for window in liste_ma_1:\n",
    "    for window_2 in liste_ma_2:\n",
    "        \n",
    "        # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "        ma_column_1 = f'ma_{window}'\n",
    "        ma_column_2 = f'ma_{window_2}'\n",
    "        position_ma_column = f'position_ma_{window_2}>{window}'\n",
    "        \n",
    "        if ma_column_1 != ma_column_2:\n",
    "            \n",
    "            # Calcul du pourcentage d'écart entre le prix et la moyenne mobile \n",
    "            data[position_ma_column] = (data[ma_column_2] > data[ma_column_1]).astype(int)\n",
    "            #MinMaxScaler_list.append(position_ma_column)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dadd44e6-b9be-4b61-a514-635d2c5856d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Position Price MA"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba663d3d-35bc-4582-aec6-a754005f44d1",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "liste_ma_1 = [9, 25, 50, 100]\n",
    "\n",
    "for window in liste_ma_1:\n",
    "    \n",
    "    # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "    ma_column_1 = f'ma_{window}'\n",
    "    position_price_ma_column = f'position_price_ma_>{window}'\n",
    "        \n",
    "    # Calcul du pourcentage d'écart entre le prix et la moyenne mobile \n",
    "    data[position_price_ma_column] = (data['Close'] > data[ma_column_1]).astype(int)\n",
    "    #MinMaxScaler_list.append(position_price_ma_column)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d9f8969-ec4e-4ab7-bde0-42514d78e028",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Volatilité"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b5852c6-9e9b-4eba-a61e-83555e26f4b2",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "returns = data['Close'].pct_change()\n",
    "\n",
    "# Condition\n",
    "if nom==\"yearly_data\" :\n",
    "    N = 1\n",
    "elif nom==\"monthly_data\":\n",
    "    N = 12\n",
    "elif nom==\"weekly_data\":\n",
    "    N=52\n",
    "else:\n",
    "    N=252\n",
    "\n",
    "# Calcul des volatilités pour chaque fenêtre\n",
    "for window in np.arange(N):\n",
    "    \n",
    "    # Calcul de la volatilité pour chaque 'Ticker'\n",
    "    vol_column = f'vol_{window}'\n",
    "    data[vol_column] = data.groupby('Ticker')['Close'].pct_change().rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True) * np.sqrt(N)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a517cff1-f5c0-4596-abe9-1da3f5f0dc8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RSI"
   ]
  },
  {
   "cell_type": "code",
   "id": "39aa4786-dd5f-42b8-8e3f-a35afea43e3c",
   "metadata": {},
   "source": [
    "\n",
    "period = 25\n",
    "\n",
    "# Variation quotidienne\n",
    "data['delta'] = data.groupby('Ticker')['Close'].diff()\n",
    "\n",
    "# Gains et pertes\n",
    "data['gains'] = data['delta'].clip(lower=0)\n",
    "data['losses'] = -data['delta'].clip(upper=0)\n",
    "\n",
    "# Moyenne mobile simple sur 'period' jours\n",
    "data['avg_gain'] = (\n",
    "    data.groupby('Ticker')['gains']\n",
    "    .transform(lambda x: x.rolling(window=period, min_periods=period).mean())\n",
    ")\n",
    "data['avg_losses'] = (\n",
    "    data.groupby('Ticker')['losses']\n",
    "    .transform(lambda x: x.rolling(window=period, min_periods=period).mean())\n",
    ")\n",
    "\n",
    "# RS et RSI\n",
    "data['rs'] = data['avg_gain'] / data['avg_losses']\n",
    "data['rsi'] = 100 - (100 / (1 + data['rs']))\n",
    "\n",
    "data = data.drop(columns=['rs', 'delta', 'avg_gain', 'avg_losses', 'gains', 'losses'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6eb2634b-dc4c-4d55-94a1-eaa938d90b90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "### ROC"
  },
  {
   "cell_type": "code",
   "id": "358d658d-a322-4b4f-9eb7-6312fdb2301b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "list_window = [4, 8, 12, 26, 52] # Liste de valeurs de top_n pour la sélection des meilleurs\n",
    "\n",
    "# Calcul des performances pour chaque fenêtre\n",
    "for window in list_window:\n",
    "    \n",
    "    # Nom dynamique de la colonne pour la performance\n",
    "    roc_column = f'roc_{window}'\n",
    "    \n",
    "    # Calcul de la performance pour chaque 'Ticker' sur la fenêtre spécifiée\n",
    "    data[roc_column] = data.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "\"\"\"\n",
    "for window in [12]:\n",
    "    # Calcul de la meilleure performance sur les mois précédents\n",
    "    data[f'best_perf_months{window}'] = data.groupby('Ticker')[f'perf_{window}'].rolling(window=window, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Momentum Ajusted Vol",
   "id": "1a1c4db4339dae0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "list_window = [4, 8, 12, 26, 52] # Liste de valeurs de top_n pour la sélection des meilleurs\n",
    "\n",
    "# Calcul des performances pour chaque fenêtre\n",
    "for window in list_window:\n",
    "\n",
    "    # Nom dynamique de la colonne pour la performance\n",
    "    MAV_column = f'Momentum_Ajusted_Vol_{window}'\n",
    "    roc_column = f'roc_{window}'\n",
    "    Vol_column = f'Vol_{window}'\n",
    "\n",
    "    # Calcul de la performance pour chaque 'Ticker' sur la fenêtre spécifiée\n",
    "    data[Vol_column] = data.groupby('Ticker')['Return'].rolling(window=window).std().reset_index(level=0, drop=True)\n",
    "    data[MAV_column] = data[roc_column] / data[Vol_column]\n"
   ],
   "id": "49bd6abe84131a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f725d244-3f20-4a3e-b6af-79dd3a352487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Distance Price"
   ]
  },
  {
   "cell_type": "code",
   "id": "40f67dd4-6b66-430a-b793-ae24d5469de3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\"\n",
    "liste_ma = [9, 25]\n",
    "for window in liste_ma:\n",
    "        \n",
    "    # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "    distance_price_column = f'distance_price_{window}'\n",
    "    ma_column = f'ma_{window}'\n",
    "\n",
    "    # Calcul du pourcentage d'écart entre le prix et la moyenne mobile \n",
    "    data[distance_price_column] = np.log(data['Close'] / data[ma_column])\n",
    "\n",
    "for window in [1, 3, 6, 12]:\n",
    "        # Calcul de la meilleure performance sur les mois précédents\n",
    "    data[f'best_distance_price_{window}'] = data.groupby('Ticker')[distance_price_column].rolling(window=window, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7eb58d17-c8eb-4610-884a-611a893684ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "### Performance"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = data.sort_values(['Ticker','Date']).copy()\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Close d’ancrage (première observation de l'année par Ticker)\n",
    "anchor_close = data.groupby(['Ticker','Year'])['Close'].transform('first')\n",
    "\n",
    "# Perf YTD (depuis le début d’année)\n",
    "data['perf_ytd'] = data['Close'] / anchor_close - 1\n",
    "data = data.drop(columns=['Year'])"
   ],
   "id": "7e12e434318745f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9eaa2abb-bad4-462f-8519-e2318490f048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "id": "f21870c4-f688-44ea-9513-4b81441b220f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "list_momentum = [1, 3, 6, 9, 12]  # Vous pouvez ajuster les fenêtres comme nécessaire\n",
    "\n",
    "for window in list_momentum:\n",
    "    # Calcul du Momentum sur une fenêtre N\n",
    "    momentum_column = f'momentum_{window}'\n",
    "    \n",
    "    # Utilisation de groupby pour appliquer un calcul de momentum de manière plus robuste\n",
    "    data[momentum_column] = data.groupby('Ticker')['Close'].transform(lambda x: x - x.shift(window))\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b82286bd-55cb-421a-9b28-3bfe8eac9f23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Changement de Volume"
   ]
  },
  {
   "cell_type": "code",
   "id": "04da1b50-8f24-4d91-8b84-5d5ada106eca",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "list_volume = [3, 6, 12]\n",
    "\n",
    "for window in list_volume:\n",
    "    \n",
    "    # Calcul du Varation du volume sur une fenêtre N\n",
    "    volume_column = f'volume{window}'\n",
    "    data[volume_column] = data.groupby('Ticker')['Volume'].pct_change(periods=window)\n",
    "    \n",
    "    # Remplacer les valeurs infinies (inf, -inf) et NaN par NaN\n",
    "    data[volume_column] = data[volume_column].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for window in [6]:\n",
    "    # Calcul de la meilleure performance sur les mois précédents\n",
    "    data[f'best_volume_{window}'] = data.groupby('Ticker')['volume12'].rolling(window=window, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5a0ae7b-d58b-4524-9090-427c54f062db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Verification des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "45fcd108-0639-478d-871e-d37da1dc0e24",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mettre toutes les valeurs NaN en 0\n",
    "data = data.dropna()\n",
    "\n",
    "# Sélectionner les colonnes numériques\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).drop(columns=['Label'])\n",
    "\n",
    "# Vérifier les valeurs infinies ou NaN\n",
    "print(numeric_columns.isna().sum())  # Compte les valeurs manquantes\n",
    "print((numeric_columns == np.inf).sum())  # Compte les valeurs infinies\n",
    "print((numeric_columns == -np.inf).sum().sort_values())  # Compte les valeurs infinies négatives\n",
    "\n",
    "data = data.replace([np.inf, -np.inf], 0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25a4e498-0172-4e5b-add3-c642b6c7e3ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modélisation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Class-Weight",
   "id": "e9fb93bcc1b9e06e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_label = len(data['Label'].unique())\n",
    "total_label = len(data['Label'])\n",
    "class_weight = {}\n",
    "for i in data['Label'].unique():\n",
    "    n = len(data[data['Label'] == i])\n",
    "    w = total_label/(n_label*n)\n",
    "    class_weight[i] = w\n",
    "\n",
    "data = data.drop(columns=['ma_9', 'ma_25', 'ma_50', 'ma_100', \"Volume\"])\n",
    "\n",
    "class_weight"
   ],
   "id": "9d22ceee798c0550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train / Test",
   "id": "e67ae12d-f06f-47bb-936f-7464a401ccb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1) Préparation des bornes temporelles\n",
    "data = data.copy()\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "cut_train_end = pd.Timestamp(\"2019-01-02\")\n",
    "cut_val_end   = pd.Timestamp(\"2023-01-02\")\n",
    "\n",
    "mask_train = data['Date'] < cut_train_end\n",
    "mask_val   = (data['Date'] >= cut_train_end) & (data['Date'] < cut_val_end)\n",
    "mask_test  = data['Date'] >= cut_val_end\n",
    "\n",
    "# --- 2) Sélection des colonnes features (en conservant l’ordre)\n",
    "drop_cols = ['Date', 'Ticker', 'Label', 'Close', 'Return']\n",
    "feature_cols = [c for c in data.columns if c not in drop_cols]\n",
    "\n",
    "X_df = data[feature_cols]\n",
    "y_sr = data['Label'].astype(int)\n",
    "\n",
    "# --- 3) Split temporel\n",
    "X_train_df, y_train = X_df.loc[mask_train], y_sr.loc[mask_train]\n",
    "X_val_df,   y_val   = X_df.loc[mask_val],   y_sr.loc[mask_val]\n",
    "X_test_df,  y_test  = X_df.loc[mask_test],  y_sr.loc[mask_test]\n",
    "\n",
    "print(\"Tailles (lignes) ->\",\n",
    "      \"train:\", len(X_train_df),\n",
    "      \"val:\", len(X_val_df),\n",
    "      \"test:\", len(X_test_df),\n",
    "      \"total:\", len(X_train_df)+len(X_val_df)+len(X_test_df),\n",
    "      \"data:\", len(data))\n",
    "\n",
    "# --- 4) Standardisation (fit UNIQUEMENT sur le train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train_df)\n",
    "X_val_scale   = scaler.transform(X_val_df)\n",
    "X_test_scale  = scaler.transform(X_test_df)\n",
    "\n",
    "# --- 6) Types optimisés pour le GPU / NN\n",
    "X_train_scale = X_train_scale.astype(np.float32)\n",
    "X_val_scale = X_val_scale.astype(np.float32)\n",
    "X_test_scale = X_test_scale.astype(np.float32)\n",
    "\n",
    "# (Optionnel) Vérifier l’équilibre des classes\n",
    "def counts(y, name):\n",
    "    vc = pd.Series(y).value_counts(normalize=True).sort_index()\n",
    "    print(f\"{name} distribution:\", vc.round(3).to_dict())\n",
    "\n",
    "counts(y_train, \"Train\")\n",
    "counts(y_val,   \"Val\")\n",
    "counts(y_test,  \"Test\")"
   ],
   "id": "d87647669f9e4eb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ad9cde7-23e8-4e4d-95f0-62d118e21057",
   "metadata": {},
   "source": [
    "print(f\"Entraînement : {X_train_scale.shape}, {y_train.shape}\")\n",
    "print(f\"Validation : {X_val_scale.shape}, {y_val.shape}\")\n",
    "print(f\"Test : {X_test_scale.shape}, {y_test.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d4b7e63-258f-4d92-818b-411e0d70d76e",
   "metadata": {},
   "source": "### Training Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "MyPath = Path(\"logs/fit\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Dossier de logs unique par session\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=str(MyPath),\n",
    "    histogram_freq=1,          # Log des histogrammes des poids\n",
    "    write_graph=True,          # Visualisation du graphe du modèle\n",
    "    write_images=True          # Affiche images d’entrée/sortie si applicable\n",
    ")\n",
    "callbacks = [early_stop, checkpoint, tensorboard_callback]\n",
    "print(\"Chemin complet :\", f\"tensorboard --logdir={MyPath.resolve()}\")"
   ],
   "id": "541091ae605f0ea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=3, max_value=15)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=8, max_value=512)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate, beta_1=0.9, beta_2=0.999,\n",
    "            epsilon=1e-08, decay=0.0\n",
    "        )\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate, momentum=0.9, nesterov=True\n",
    "        )\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation=\"swish\", kernel_initializer=\"he_uniform\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "        model.add(layers.BatchNormalization(momentum=0.9))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\", dtype='float32'))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ],
   "id": "c8c3540f361ee229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_search_tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,\n",
    "    directory=\"DNN_SP500\",\n",
    "    project_name=\"SP500_Weekly_to_Yearly1_Increase_TOP10_Full_Data\",\n",
    "    seed=42,\n",
    "\n",
    ")"
   ],
   "id": "8b27d455489aa311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_search_tuner.search(X_train_scale, y_train, epochs=100, callbacks=callbacks, validation_data=(X_val_scale, y_val), batch_size=32768, class_weight=class_weight)",
   "id": "6fde790c5abe54d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = random_search_tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = random_search_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(best_hps.values)"
   ],
   "id": "dc8517523001e144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BATCH = 65536\n",
    "test_ds = (tf.data.Dataset.from_tensor_slices(X_test_scale)\n",
    "           .batch(BATCH)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices(X_train_scale)\n",
    "           .batch(BATCH)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\"\"\"\n",
    "def mc_dropout_predictions(model, X, n_samples=100):\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        y_pred = model(X, training=True)  # Dropout actif\n",
    "        preds.append(y_pred.numpy().flatten())\n",
    "    return np.array(preds)\n",
    "\n",
    "# Exécution sur X_test_scale\n",
    "preds_mc = mc_dropout_predictions(best_model, X_test_scale, n_samples=50)\n",
    "\n",
    "# Moyenne et écart-type des probabilités\n",
    "probas_mean = preds_mc.mean(axis=0)\n",
    "probas_std = preds_mc.std(axis=0)\n",
    "\n",
    "# Exemple d’affichage\n",
    "print(\"Probabilités moyennes :\", probas_mean[:10])\n",
    "print(\"Incertitudes (std) :\", probas_std[:10])\n",
    "\"\"\"\n",
    "y_pred_proba = best_model.predict(test_ds)\n",
    "y_pred_train = best_model.predict(train_ds)"
   ],
   "id": "1ae0a90e0239cf8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_pred_test = (y_pred_proba > 0.5).astype(int)",
   "id": "ff2503154c6cc2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dfb428f8-2022-4b0f-a9fb-1adf312147f4",
   "metadata": {},
   "source": "# Evaluation Model"
  },
  {
   "cell_type": "markdown",
   "id": "8e2e9749-1e5b-4522-a558-6e9ac055926e",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Accuracy Score\n",
    "acc_train = accuracy_score(y_true=np.array(y_train), y_pred=np.array(y_pred_train).round())\n",
    "acc_test = accuracy_score(y_true=np.array(y_test), y_pred=np.array(y_pred_test).round())\n",
    "print(\"ACC TRAIN\", acc_train)\n",
    "print(\"ACC TEST\", acc_test)"
   ],
   "id": "7788faf67faf5d1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a860a067-e6d1-4c19-9a26-cfea55e60cbd",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43b723d3-6523-4bb0-8c0d-bcee6ec10501",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "id": "d02f05b6-f9aa-42a3-933f-191ce23059e8",
   "metadata": {},
   "source": [
    "# Rapport de classification\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "#print(classification_report(y_train_res, y_pred_train))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27a41421-c3ce-4f43-a395-8ccc73c7e728",
   "metadata": {},
   "source": [
    "### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "id": "dabda619-9faa-42fd-a74c-5ddffa07ce39",
   "metadata": {},
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "cm_display.plot(cmap=\"Blues\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "999ec319-ecf6-4efc-b28a-50acbf976fc0",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "id": "89234d6d-ebc8-4ddf-95d3-b801e9d1b3d9",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Forcer l'exécution sur CPU\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "# Nombre d'arbres de ta forêt\n",
    "n_estimators = 300\n",
    "\n",
    "# Créer un modèle en mode \"warm_start\" (permet d’ajouter les arbres un par un)\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=1,  # on commence avec 1 arbre\n",
    "        warm_start=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entraînement incrémental avec tqdm\n",
    "    for i in tqdm(range(1, n_estimators + 1), desc=\"Entraînement RF\"):\n",
    "        rf.n_estimators = i\n",
    "        rf.fit(X_train_df, y_train)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "# Importance des features\n",
    "importance_rf = rf.feature_importances_\n",
    "\n",
    "# Afficher les résultats\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X_train_df.columns,\n",
    "    'Importance': importance_rf\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance_rf.head(20)\n",
    "\"\"\""
   ],
   "id": "182fba5fea23ea9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0086beac-1e2c-404d-acae-c93c62663b06",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Visualiser l'importance des features\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.barh(feature_importance_rf['Feature'], feature_importance_rf['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance des Features')\n",
    "plt.title('Importance des Features dans le Modèle Random Forest')\n",
    "plt.show()\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PCA",
   "id": "218962a99b3809c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_res = scaler.fit_transform(X_res)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(feature_cols))\n",
    "X_pca = pca.fit_transform(X_train_scale)\n",
    "\n",
    "# Get the components (loadings)\n",
    "pcs = pca.components_\n",
    "\n",
    "# Get the feature names\n",
    "features =feature_cols\n",
    "\n",
    "# Correlation circle\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.axhline(0, color='grey', lw=1)\n",
    "plt.axvline(0, color='grey', lw=1)\n",
    "\n",
    "# Draw unit circle\n",
    "circle = plt.Circle((0, 0), 1, color='black', fill=False, linestyle='--')\n",
    "plt.gca().add_artist(circle)\n",
    "\n",
    "# Plot arrows for each feature\n",
    "for i, feature in enumerate(features):\n",
    "    plt.arrow(0, 0, pcs[0, i], pcs[1, i], head_width=0.03, head_length=0.03, color='r')\n",
    "    plt.text(pcs[0, i]*1.1, pcs[1, i]*1.1, feature, color='black', ha='center', va='center')\n",
    "\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.title(\"Correlation Circle (PCA)\")\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "id": "fb441eeab519fca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scale)  # X doit être centré-réduit !\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(cumulative_var, marker='o')\n",
    "plt.xlabel(\"Nombre de composantes\")\n",
    "plt.ylabel(\"Variance expliquée cumulée\")\n",
    "plt.title(\"Variance expliquée par PCA\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "febc1d603fb39ae7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d42ee81d-41ed-4297-b3a5-0cf7c993fabd",
   "metadata": {},
   "source": [
    "### Courbe ROC - AUC"
   ]
  },
  {
   "cell_type": "code",
   "id": "b22f53d5-62fc-4c60-b1f5-b7e47959533e",
   "metadata": {},
   "source": [
    "# Calcul des valeurs pour la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n",
    "\n",
    "# Calcul de l'AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='b', label=f'Courbe ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Ligne de référence (modèle aléatoire)\n",
    "plt.xlabel('Taux de faux positifs (FPR)')\n",
    "plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03a46941-12e7-418b-926d-6c04c281dd4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Total Return Model Past Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ede42f-9985-423a-8eba-e793e28f627f",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_Alpha_Jensen(portfolio_returns: pd.Series,\n",
    "                          benchmark_returns: pd.Series,\n",
    "                          risk_free_rate: float = 0.2\n",
    "                         ) -> float:\n",
    "\n",
    "    # 2) Estimation de beta\n",
    "    cov_pm = np.cov(portfolio_returns, benchmark_returns, ddof=0)[0, 0]\n",
    "    var_m  = np.var(benchmark_returns, ddof=0)\n",
    "    beta   = cov_pm / var_m\n",
    "\n",
    "    # 3) Rendement moyen\n",
    "    mean_port = np.mean(portfolio_returns)\n",
    "    mean_bench = np.mean(benchmark_returns)\n",
    "\n",
    "    # 4) Calcul de l'alpha\n",
    "    expected_port = risk_free_rate + beta * (mean_bench - risk_free_rate)\n",
    "    alpha = mean_port - expected_port\n",
    "\n",
    "    return alpha"
   ],
   "id": "51df4d89e93b8b99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_CAGR(portfolio_returns, annual_returns):\n",
    "\n",
    "    # Paramètres\n",
    "    V_fin = portfolio_returns.iloc[-1]\n",
    "    V_debut = initiale_capital\n",
    "    t = annual_returns\n",
    "\n",
    "    # Calculer du CAGR\n",
    "    CAGR = (((V_fin / V_debut) ** (1 / t)) -1) * 100\n",
    "\n",
    "    return CAGR"
   ],
   "id": "af996aef-1176-49c5-b03f-e37a7b77a08c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_Volatility(returns, periods_per_year = 12):\n",
    "\n",
    "    vol = returns.std(ddof=1)\n",
    "    vol *= np.sqrt(periods_per_year)\n",
    "\n",
    "    return vol"
   ],
   "id": "89c1d494504de6ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_Sharpe_Ratio(returns, vol, risk_free_rate=0.02, periods_per_year=12):\n",
    "\n",
    "    returns = Calculate_CAGR(returns, len(returns))\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "    mean_exc = excess_returns.mean() * periods_per_year\n",
    "\n",
    "    return mean_exc / Calculate_Volatility(vol)"
   ],
   "id": "2a5bb656-f0fd-40f0-9dc3-62b5bbd6be2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_Total_Returns(portfolio_returns):\n",
    "\n",
    "    Value_start = initiale_capital\n",
    "    Value_final = portfolio_returns.iloc[-1]\n",
    "\n",
    "    Total_Returns = (Value_final - Value_start) / Value_start\n",
    "\n",
    "    return Total_Returns"
   ],
   "id": "2c8bb58c-b3f5-4291-a538-c1b1ed9adf1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def Calculate_Max_Drawdown(annual_returns):\n",
    "\n",
    "    maxdrawdown = annual_returns.min()\n",
    "\n",
    "    return maxdrawdown"
   ],
   "id": "500fd5a0-e1c2-4da1-a94f-dd85ed45ada5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_data = pd.DataFrame()\n",
    "data_SP500 = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_SP500.parquet')\n",
    "data_NDX = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_NASDAQ.parquet')\n",
    "data_MP = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_MP.parquet')\n",
    "data_Crypto = pd.read_parquet('/Users/forget/Library/Mobile Documents/com~apple~CloudDocs/Project Stock Market Deep Learning/Data/data_Crypto.parquet')\n",
    "daily_data = pd.concat([data_SP500, data_NDX, data_MP, data_Crypto], ignore_index=True)\n",
    "daily_data = daily_data.drop_duplicates(subset=['Ticker', 'Date'])\n",
    "\n",
    "daily_data = daily_data.drop(columns=['Adj Close'])\n",
    "# trier la data pour préparer la fusion\n",
    "daily_data = daily_data.sort_values(by=['Date'], ascending=[True])\n",
    "\n",
    "#Date au format Date\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "# Définir la colonne 'Date' comme index\n",
    "daily_data.set_index('Date', inplace=True)\n",
    "daily_data"
   ],
   "id": "d0502d8d59aea7f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weekly_data = daily_data.groupby(\"Ticker\").resample('W').agg({\n",
    "    'Open': 'first',   # Premier prix d'ouverture du mois\n",
    "    'High': 'max',     # Plus haut du mois\n",
    "    'Low': 'min',      # Plus bas du mois\n",
    "    'Close': 'last',   # Dernier prix de clôture du mois\n",
    "    'Volume': 'sum',   # Somme du volume sur le mois\n",
    "}).reset_index()"
   ],
   "id": "adc220f4764bf2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculer le rendement pour chaque Ticker avec un décalage d'un mois\n",
    "weekly_data['Return'] = (weekly_data['Close'] / weekly_data['Open']) -1\n",
    "\n",
    "# Paramètres\n",
    "liste_ma = [9, 20, 25, 50, 100]\n",
    "\n",
    "# Calcul des moyennes mobiles pour chaque fenêtre\n",
    "for window in liste_ma:\n",
    "    # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "    ma_column = f'ma_{window}'\n",
    "    weekly_data[ma_column] = weekly_data.groupby('Ticker')['Close'].transform(lambda s: s.rolling(window, min_periods=1).mean())\n",
    "\n",
    "liste_ma_1 = [9, 25, 50, 100]\n",
    "liste_ma_2 = [9, 25, 50, 100]\n",
    "\n",
    "for window in liste_ma_1:\n",
    "    for window_2 in liste_ma_2:\n",
    "\n",
    "        # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "        ma_column_1 = f'ma_{window}'\n",
    "        ma_column_2 = f'ma_{window_2}'\n",
    "        distance_ma_column = f'distance_ma_{window_2}/{window}'\n",
    "\n",
    "        if ma_column_1 != ma_column_2:\n",
    "            # Calcul du pourcentage d'écart entre le prix et la moyenne mobile\n",
    "            weekly_data[distance_ma_column] = weekly_data[ma_column_2] / weekly_data[ma_column_1]\n",
    "\n",
    "\n",
    "list_window = [4, 8, 12, 26, 52] # Liste de valeurs de top_n pour la sélection des meilleurs\n",
    "\n",
    "# Calcul des performances pour chaque fenêtre\n",
    "for window in list_window:\n",
    "\n",
    "    # Nom dynamique de la colonne pour la performance\n",
    "    roc_column = f'roc_{window}'\n",
    "\n",
    "    # Calcul de la performance pour chaque 'Ticker' sur la fenêtre spécifiée\n",
    "    weekly_data[roc_column] = weekly_data.groupby('Ticker')['Close'].pct_change(window)\n",
    "\n",
    "liste_ma = [9, 25]\n",
    "for window in liste_ma:\n",
    "\n",
    "    # Calcul de la moyenne mobile pour chaque 'Ticker'\n",
    "    distance_price_column = f'distance_price_{window}'\n",
    "    ma_column = f'ma_{window}'\n",
    "\n",
    "    # Calcul du pourcentage d'écart entre le prix et la moyenne mobile\n",
    "    weekly_data[distance_price_column] = weekly_data['Close'] / weekly_data[ma_column]\n",
    "\n",
    "weekly_data['Year'] = weekly_data['Date'].dt.year\n",
    "\n",
    "# Close d’ancrage (première observation de l'année par Ticker)\n",
    "anchor_close = weekly_data.groupby(['Ticker','Year'])['Close'].transform('first')\n",
    "\n",
    "# Perf YTD (depuis le début d’année)\n",
    "weekly_data['perf_ytd'] = weekly_data['Close'] / anchor_close - 1\n",
    "\n",
    "list_window = [4, 8, 12, 26, 52] # Liste de valeurs de top_n pour la sélection des meilleurs\n",
    "\n",
    "# Calcul des performances pour chaque fenêtre\n",
    "for window in list_window:\n",
    "\n",
    "    # Nom dynamique de la colonne pour la performance\n",
    "    MAV_column = f'Momentum_Ajusted_Vol_{window}'\n",
    "    Perf_column = f'Perf_{window}'\n",
    "    Vol_column = f'Vol_{window}'\n",
    "\n",
    "    # Calcul de la performance pour chaque 'Ticker' sur la fenêtre spécifiée\n",
    "    weekly_data[Perf_column] = weekly_data.groupby('Ticker')['Close'].pct_change(window)\n",
    "    weekly_data[Vol_column] = weekly_data.groupby('Ticker')['Return'].rolling(window=window).std().reset_index(level=0, drop=True)\n",
    "    weekly_data[MAV_column] = weekly_data[Perf_column] / weekly_data[Vol_column]\n",
    "\n",
    "period = 25\n",
    "\n",
    "# Variation quotidienne\n",
    "weekly_data['delta'] = weekly_data.groupby('Ticker')['Close'].diff()\n",
    "\n",
    "# Gains et pertes\n",
    "weekly_data['gains'] = weekly_data['delta'].clip(lower=0)\n",
    "weekly_data['losses'] = -weekly_data['delta'].clip(upper=0)\n",
    "\n",
    "# Moyenne mobile simple sur 'period' jours\n",
    "weekly_data['avg_gain'] = (\n",
    "    weekly_data.groupby('Ticker')['gains']\n",
    "    .transform(lambda x: x.rolling(window=period, min_periods=period).mean())\n",
    ")\n",
    "weekly_data['avg_losses'] = (\n",
    "    weekly_data.groupby('Ticker')['losses']\n",
    "    .transform(lambda x: x.rolling(window=period, min_periods=period).mean())\n",
    ")\n",
    "\n",
    "# RS et RSI\n",
    "weekly_data['rs'] = weekly_data['avg_gain'] / weekly_data['avg_losses']\n",
    "weekly_data['rsi'] = 100 - (100 / (1 + weekly_data['rs']))\n",
    "\n",
    "#weekly_data = weekly_data.drop(columns=['Return', 'ma_9', 'ma_25', 'ma_50', 'ma_100'])\n",
    "weekly_data = weekly_data.dropna()"
   ],
   "id": "bc4fd6aaba908ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "start_date = \"1990-01-01\"\n",
    "\n",
    "weekly_data = weekly_data[weekly_data['Date'] >= start_date]\n",
    "\n",
    "weekly_data = weekly_data.replace([np.inf, -np.inf], 0)\n",
    "#weekly_data = weekly_data.drop(columns=['Close', 'Open', 'High', 'Low', 'Volume'])\n",
    "# Remplacer les valeurs NaN par 0\n",
    "weekly_data = weekly_data.dropna()\n",
    "\n",
    "weekly_data"
   ],
   "id": "a8a32fec-36f9-43e6-98c8-0053f5d327be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 1) Télécharger les cours mensuels ajustés du S&P 500\n",
    "ndx = yf.download(\n",
    "    \"^GSPC\",\n",
    "    start=\"1976-07-01\",\n",
    "    end=\"2025-07-17\",\n",
    "    interval=\"1wk\",\n",
    "    auto_adjust=True,\n",
    "    actions=False\n",
    ")[[\"Open\", \"Close\"]]\n",
    "\n",
    "ndx.columns = ndx.columns.droplevel(level=1)\n",
    "\n",
    "ndx.columns = ['ndx_Open', 'ndx_Close']\n",
    "\n",
    "# 2) Convertir l’index en période mensuelle, puis au TIMESTAMP de fin de mois\n",
    "ndx.index = pd.to_datetime(ndx.index)\n",
    "\n",
    "# 3) Normaliser pour que le S&P 500 commence à la même valeur que le capital initial\n",
    "ndx['Return'] = (ndx['ndx_Close'].pct_change()).fillna(0)\n",
    "\n",
    "# Calcul des moyennes mobiles pour chaque fenêtre\n",
    "ndx['Slow_ma'] = ndx['ndx_Close'].rolling(window=25, min_periods=1).mean()\n",
    "ndx['Fast_ma'] = ndx['ndx_Close'].rolling(window=9, min_periods=1).mean()\n",
    "\n",
    "crisis = []\n",
    "for i in range(0, len(ndx)):\n",
    "    if ndx['Slow_ma'].iloc[i] < ndx['Fast_ma'].iloc[i] and ndx['Slow_ma'].iloc[i] < ndx['ndx_Close'].iloc[i]:\n",
    "        crisis.append(0)\n",
    "    elif ndx['Slow_ma'].iloc[i] > ndx['Fast_ma'].iloc[i] and ndx['Fast_ma'].iloc[i] < ndx['ndx_Close'].iloc[i]:\n",
    "        crisis.append(0)\n",
    "    else:\n",
    "        crisis.append(0)\n",
    "\n",
    "ndx['Crisis'] = crisis\n",
    "\n",
    "ndx = ndx.fillna(0)\n",
    "ndx = ndx.drop(columns=['ndx_Open', 'ndx_Close','Return', 'Slow_ma', 'Fast_ma'])\n",
    "\n",
    "weekly_data = pd.merge_asof(\n",
    "    weekly_data.sort_values('Date'),\n",
    "    ndx.sort_values('Date'),\n",
    "    on='Date',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "#data = data[data['Crisis'] == 0]\n"
   ],
   "id": "71134156e3acfe9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#data = data.drop(columns=['distance_ma_25/9', 'distance_ma_50/9', 'distance_ma_100/9', 'distance_ma_25/50', 'distance_ma_50/100', 'distance_ma_25/100'])",
   "id": "6e3d13625c52ec59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_cols",
   "id": "3e58e2b78a38e86e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = weekly_data[[\n",
    "'distance_ma_25/9',\n",
    " 'distance_ma_50/9',\n",
    " 'distance_ma_100/9',\n",
    " 'distance_ma_9/25',\n",
    " 'distance_ma_50/25',\n",
    " 'distance_ma_100/25',\n",
    " 'distance_ma_9/50',\n",
    " 'distance_ma_25/50',\n",
    " 'distance_ma_100/50',\n",
    " 'distance_ma_9/100',\n",
    " 'distance_ma_25/100',\n",
    " 'distance_ma_50/100',\n",
    " 'rsi',\n",
    " 'roc_4',\n",
    " 'roc_8',\n",
    " 'roc_12',\n",
    " 'roc_26',\n",
    " 'roc_52',\n",
    " 'Vol_4',\n",
    " 'Momentum_Ajusted_Vol_4',\n",
    " 'Vol_8',\n",
    " 'Momentum_Ajusted_Vol_8',\n",
    " 'Vol_12',\n",
    " 'Momentum_Ajusted_Vol_12',\n",
    " 'Vol_26',\n",
    " 'Momentum_Ajusted_Vol_26',\n",
    " 'Vol_52',\n",
    " 'Momentum_Ajusted_Vol_52',\n",
    " 'perf_ytd']]\n",
    "\n",
    "X_scale_test = scaler.fit_transform(X)\n",
    "weekly_data['Proba'] = best_model.predict(X_scale_test).ravel()\n"
   ],
   "id": "2f1dd2b6-d14d-45d3-a10b-1bc10aefd5d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weekly_data['Prediction'] = (weekly_data['Proba'] > 0.5).astype(int)\n",
    "weekly_data['Prediction'].value_counts()"
   ],
   "id": "3700b323cb3f5725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "cell_type": "markdown",
   "source": "### Backtest",
   "id": "f1e92632-e414-4aed-8a1b-4c17df8c21fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = weekly_data.copy()",
   "id": "7fca3efbddc5dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def backtest(\n",
    "    data: pd.DataFrame,\n",
    "    initial_capital: float,\n",
    "    save_month: float,\n",
    "    fee_per_trade: float,\n",
    "    moving_average_buy: int,\n",
    "    moving_average_sell: int,\n",
    "    proba_column: str = \"Proba\",\n",
    "    max_positions: int = 5,   # plafond de positions\n",
    "):\n",
    "    \"\"\"\n",
    "    Colonnes requises dans `data` :\n",
    "      - Date (datetime64)\n",
    "      - Ticker (str)\n",
    "      - Open (float), Close (float)\n",
    "      - Prediction (0/1)\n",
    "      - Return (float)       → rendement de la période SUIVANTE (t+1)\n",
    "      - ma_{moving_average_buy}, ma_{moving_average_sell}\n",
    "      - Crisis (0/1)         → 1 = crise (on reste cash), 0 = normal\n",
    "      - Proba (float)        → optionnelle, sert à pondérer / classer\n",
    "    \"\"\"\n",
    "\n",
    "    # --- vérifs & pré-tri ---\n",
    "    ma_buy  = f\"ma_{moving_average_buy}\"\n",
    "    ma_sell = f\"ma_{moving_average_sell}\"\n",
    "    required = [\"Date\",\"Ticker\",\"Open\",\"Close\",\"Prediction\",\"Return\",ma_buy,ma_sell,\"Crisis\"]\n",
    "    missing = [c for c in required if c not in data.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "    df = data.sort_values([\"Date\", \"Ticker\"]).copy()\n",
    "    unique_dates = np.sort(df[\"Date\"].unique())\n",
    "\n",
    "    # --- états backtest ---\n",
    "    capital = float(initial_capital)\n",
    "    previous_portfolio: set[str] = set()\n",
    "\n",
    "    capital_hist, ret_hist = [], []\n",
    "    count_hist, fees_hist = [], []\n",
    "    opened_hist, closed_hist, total_hist = [], [], []\n",
    "    tickers_history: dict = {}\n",
    "\n",
    "    # --- boucle principale ---\n",
    "    for i, date in enumerate(unique_dates):\n",
    "        capital += float(save_month)\n",
    "\n",
    "        # Slice du jour\n",
    "        cols_today = [\"Ticker\",\"Open\",\"Close\",\"Prediction\",\"Return\",\"Crisis\",ma_buy,ma_sell, 'rsi']\n",
    "        if proba_column in df.columns:\n",
    "            cols_today.append(proba_column)\n",
    "        today = df.loc[df[\"Date\"] == date, cols_today].copy()\n",
    "\n",
    "        if today.empty:\n",
    "            # sécurité (au cas improbable)\n",
    "            capital_hist.append(capital)\n",
    "            ret_hist.append(0.0)\n",
    "            count_hist.append(len(previous_portfolio))\n",
    "            fees_hist.append(0.0)\n",
    "            opened_hist.append(0)\n",
    "            closed_hist.append(0)\n",
    "            total_hist.append(0)\n",
    "            continue\n",
    "\n",
    "        crisis_flag = int(today[\"Crisis\"].iloc[0])\n",
    "\n",
    "        # --------- déterminer le portefeuille courant ---------\n",
    "        if crisis_flag == 1:\n",
    "            # En crise : on vend tout, reste en cash\n",
    "            current_portfolio = set()\n",
    "            #sell_set = previous_portfolio  # tout doit être vendu\n",
    "            #buy_set = set()\n",
    "        else:\n",
    "            # Pas de crise → règles normales BUY/SELL\n",
    "            buy_candidates = today.loc[\n",
    "                (today[ma_buy] < today[\"Open\"]) &\n",
    "                (today[ma_buy] < today[\"Close\"]) &\n",
    "                (today[\"Prediction\"] == 1),\n",
    "                \"Ticker\"\n",
    "            ]\n",
    "            sell_candidates = today.loc[(\n",
    "                    (today[ma_sell] > today[\"Open\"]) &\n",
    "                    (today[ma_sell] > today[\"Close\"])),\n",
    "                \"Ticker\"\n",
    "            ]\n",
    "            buy_set  = set(buy_candidates)\n",
    "            sell_set = set(sell_candidates)\n",
    "\n",
    "            # Portefeuille après ventes\n",
    "            held_after_sell = previous_portfolio - sell_set\n",
    "            new_buys = buy_set - previous_portfolio\n",
    "\n",
    "            # ----- classement par score pour bornage à max_positions -----\n",
    "            # Score = Proba si dispo/valide, sinon momentum simple\n",
    "            today_idx = today.set_index(\"Ticker\")\n",
    "            if (proba_column in today_idx.columns) and today_idx[proba_column].notna().any():\n",
    "                score_map = today_idx[proba_column].fillna(0.0).to_dict()\n",
    "            else:\n",
    "                # momentum proxy : Close/ma_buy - 1 (plus c'est grand, mieux c'est)\n",
    "                mom = (today_idx[\"Close\"] / today_idx[ma_buy] - 1.0)\n",
    "                mom = mom.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "                score_map = mom.to_dict()\n",
    "\n",
    "            # 1) on garde les positions détenues (après ventes), triées par score\n",
    "            held_sorted = sorted(list(held_after_sell), key=lambda t: score_map.get(t, 0.0), reverse=True)\n",
    "\n",
    "            if len(held_sorted) >= max_positions:\n",
    "                # Trop de positions : on coupe les moins bien classées\n",
    "                current_portfolio = set(held_sorted[:max_positions])\n",
    "            else:\n",
    "                # On complète avec les nouveaux BUY les mieux classés\n",
    "                remaining = max_positions - len(held_sorted)\n",
    "                new_sorted = sorted(list(new_buys), key=lambda t: score_map.get(t, 0.0), reverse=True)\n",
    "                add_these = new_sorted[:remaining]\n",
    "                current_portfolio = set(held_sorted + add_these)\n",
    "\n",
    "        # --------- frais fixes (ouvertures / fermetures) ---------\n",
    "        opened = current_portfolio - previous_portfolio\n",
    "        closed = previous_portfolio - current_portfolio\n",
    "        n_opened = len(opened)\n",
    "        n_closed = len(closed)\n",
    "        n_total  = n_opened + n_closed\n",
    "        fees_paid = n_total * float(fee_per_trade)\n",
    "        capital -= fees_paid  # débit immédiat\n",
    "\n",
    "        # --------- préparer les retours t+1 ---------\n",
    "        holdings = pd.DataFrame({\"Ticker\": list(current_portfolio)})\n",
    "        if i + 1 < len(unique_dates) and not holdings.empty:\n",
    "            next_date = unique_dates[i + 1]\n",
    "            next_ret = df.loc[df[\"Date\"] == next_date, [\"Ticker\",\"Return\"]]\n",
    "\n",
    "            merged = holdings.merge(next_ret, on=\"Ticker\", how=\"left\")\n",
    "\n",
    "            # Ajoute la proba du jour pour pondérer si dispo\n",
    "            if proba_column in today.columns:\n",
    "                merged = merged.merge(today[[\"Ticker\", proba_column]], on=\"Ticker\", how=\"left\")\n",
    "\n",
    "            # On ne garde que ceux qui ont un Return observé à t+1\n",
    "            merged = merged.dropna(subset=[\"Return\"]).reset_index(drop=True)\n",
    "            # Met à jour le portefeuille (au cas où certains titres n’ont pas de Return)\n",
    "            current_portfolio = set(merged[\"Ticker\"])\n",
    "\n",
    "            # Poids cibles\n",
    "            r = merged[\"Return\"].astype(float).to_numpy()\n",
    "            if proba_column in merged.columns:\n",
    "                p = merged[proba_column].fillna(0.0).astype(float).to_numpy()\n",
    "                w = (p / p.sum()) if p.sum() > 0 else np.ones_like(r) / max(len(r), 1)\n",
    "            else:\n",
    "                w = np.ones_like(r) / max(len(r), 1)\n",
    "\n",
    "            mean_return = float(np.dot(r, w))\n",
    "        else:\n",
    "            # pas de prochaine date ou portefeuille vide → pas de perf\n",
    "            merged = holdings.copy()\n",
    "            merged[\"Return\"] = 0.0\n",
    "            if proba_column in today.columns:\n",
    "                merged[proba_column] = np.nan\n",
    "            mean_return = 0.0\n",
    "\n",
    "        # --------- appliquer rendement ---------\n",
    "        capital *= (1.0 + mean_return)\n",
    "\n",
    "        # --------- historique ---------\n",
    "        tickers_history[date] = merged.copy()\n",
    "        capital_hist.append(capital)\n",
    "        ret_hist.append(mean_return)\n",
    "        count_hist.append(len(current_portfolio))\n",
    "        fees_hist.append(fees_paid)\n",
    "        opened_hist.append(n_opened)\n",
    "        closed_hist.append(n_closed)\n",
    "        total_hist.append(n_total)\n",
    "\n",
    "        # t devient t-1\n",
    "        previous_portfolio = current_portfolio\n",
    "\n",
    "    # --------- sorties agrégées ---------\n",
    "    portfolio_per_day = pd.DataFrame({\n",
    "        \"Date\": unique_dates,\n",
    "        \"Capital\": capital_hist,\n",
    "        \"Return\": np.array(ret_hist) * 100.0,\n",
    "        \"Ticker_Count\": count_hist,\n",
    "        \"Fees\": fees_hist,\n",
    "        \"Trades_Opened\": opened_hist,\n",
    "        \"Trades_Closed\": closed_hist,\n",
    "        \"Trades_Total\": total_hist,\n",
    "    }).set_index(\"Date\")\n",
    "\n",
    "    # Mensuel\n",
    "    monthly = portfolio_per_day.resample(\"ME\").agg({\n",
    "        \"Capital\": \"last\",\n",
    "        \"Ticker_Count\": \"mean\",\n",
    "        \"Fees\": \"sum\",\n",
    "        \"Trades_Opened\": \"sum\",\n",
    "        \"Trades_Closed\": \"sum\",\n",
    "        \"Trades_Total\": \"sum\",\n",
    "    })\n",
    "    first_cap_m = portfolio_per_day[\"Capital\"].resample(\"ME\").first().ffill()\n",
    "    last_cap_m  = portfolio_per_day[\"Capital\"].resample(\"ME\").last().ffill()\n",
    "    monthly[\"Return\"] = ((last_cap_m / first_cap_m - 1.0) * 100.0).fillna(0.0)\n",
    "\n",
    "    # Annuel\n",
    "    annual = portfolio_per_day.resample(\"YE\").agg({\n",
    "        \"Capital\": \"last\",\n",
    "        \"Ticker_Count\": \"mean\",\n",
    "        \"Fees\": \"sum\",\n",
    "        \"Trades_Opened\": \"sum\",\n",
    "        \"Trades_Closed\": \"sum\",\n",
    "        \"Trades_Total\": \"sum\",\n",
    "    })\n",
    "    first_cap_y = portfolio_per_day[\"Capital\"].resample(\"YE\").first().ffill()\n",
    "    last_cap_y  = portfolio_per_day[\"Capital\"].resample(\"YE\").last().ffill()\n",
    "    annual[\"Return\"] = ((last_cap_y / first_cap_y - 1.0) * 100.0).fillna(0.0)\n",
    "\n",
    "    return portfolio_per_day, monthly, annual, tickers_history"
   ],
   "id": "c9677695164070a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initiale_capital = 1000\n",
    "portfolio_per_day, portfolio_per_month, portfolio_per_annual, list_ticker = backtest(weekly_data, initial_capital=initiale_capital, save_month=0, fee_per_trade=1, moving_average_buy=9, moving_average_sell=25)"
   ],
   "id": "b8274a7aa408e82c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "worst = portfolio_per_month[portfolio_per_month['Return'] < 0]\n",
    "worst = worst.sort_values(by=['Return'], ascending=True)\n",
    "print(len(portfolio_per_month), len(worst), len(worst)/len(portfolio_per_month)*100)"
   ],
   "id": "30fc9c89cb5a6865",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "worst.head(10)",
   "id": "a829bbdf44bfcb84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "portfolio_per_day['Ticker_Count'].describe()",
   "id": "3a6c11db754a0d7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "portfolio_per_annual",
   "id": "67abcf7b87ad6c4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_ticker",
   "id": "4720a2abfe312604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_ticker.items()",
   "id": "f06c3143e96abb6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Calculer le drawdown mensuel\n",
    "#    drawdown = (Capital / pic_historique) - 1\n",
    "portfolio_per_day['Peak'] = portfolio_per_day['Capital'].cummax()\n",
    "portfolio_per_day['Drawdown'] = portfolio_per_day['Capital'] / portfolio_per_day['Peak'] - 1\n",
    "\n",
    "# 2) Résultat : série mensuelle de drawdowns\n",
    "#    Le drawdown est négatif (0 au pic, baisse ensuite)  [oai_citation:0‡reddit.com](https://www.reddit.com/r/learnpython/comments/bxyze5/getting_max_drawdown_with_python/?utm_source=chatgpt.com)\n",
    "\n",
    "# 3) Calculer le max drawdown global\n",
    "max_dd_global = portfolio_per_day['Drawdown'].min()\n",
    "\n",
    "# 4) Calculer le drawdown maximal par année\n",
    "#    On regroupe par année et on prend le minimum (le plus négatif) de Drawdown\n",
    "mensuel_max_dd = (\n",
    "    portfolio_per_day['Drawdown']\n",
    "    .groupby(portfolio_per_day.index.year)\n",
    "    .min()\n",
    "    .rename('MaxDrawdown')\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "# Calcul du nombre d'actions sélectionnées chaque mois\n",
    "monthly_counts = data.groupby('Date')['Prediction'].sum()\n",
    "monthly_counts.index = pd.to_datetime(monthly_counts.index)\n",
    "\n",
    "# Moyenne des sélections mensuelles\n",
    "average_count = monthly_counts.mean()\n",
    "\n",
    "# 1) Regrouper par année (resample('Y') pour Year-End)\n",
    "annual_counts = monthly_counts.resample('YE').sum()\n",
    "\n",
    "# 2) Calculer la moyenne annuelle des sélections\n",
    "average_annual_count = annual_counts.mean()\n"
   ],
   "id": "dc7de44c283bcd37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = data.sort_values('Date')\n",
    "# 1) Télécharger les cours mensuels ajustés du S&P 500\n",
    "ndx = yf.download(\n",
    "    \"^GSPC\",\n",
    "    start=data['Date'].iloc[0],\n",
    "    end=data['Date'].iloc[-1],\n",
    "    interval=\"1wk\",\n",
    "    auto_adjust=True,\n",
    "    actions=False\n",
    ")[\"Close\"]\n",
    "# 2) Convertir l’index en période mensuelle, puis au TIMESTAMP de fin de mois\n",
    "ndx.index = pd.to_datetime(ndx.index)\n",
    "\n",
    "# 3) Normaliser pour que le S&P 500 commence à la même valeur que le capital initial\n",
    "ndx_return = (ndx.pct_change())\n",
    "\n",
    "# 4) Reindexer ndx_norm sur l’index de portfolio_per_month (remplit les mois manquants par propagation)\n",
    "ndx_return = ndx_return.reindex(portfolio_per_day.index, method=\"ffill\")\n",
    "\n",
    "# 5) Fusionner dans df_bench\n",
    "portfolio_per_day[\"S&P500_return\"] = ndx_return\n",
    "portfolio_per_day[\"S&P500_return\"].fillna(0)\n",
    "\n",
    "initial_cap_ndx = portfolio_per_day[\"Capital\"].iloc[0]\n",
    "portfolio_per_day[\"S&P500_capital\"] = (1 + portfolio_per_day[\"S&P500_return\"]).cumprod() * initial_cap_ndx\n",
    "portfolio_per_day['S&P500_return'] = portfolio_per_day['S&P500_return'] * 100\n",
    "\n",
    "# 1) Calculer le drawdown mensuel\n",
    "#    drawdown = (Capital / pic_historique) - 1\n",
    "portfolio_per_day['S&P500_Peak'] = portfolio_per_day['S&P500_capital'].cummax()\n",
    "portfolio_per_day['S&P500_Drawdown'] = portfolio_per_day['S&P500_capital'] / portfolio_per_day['S&P500_Peak'] - 1\n",
    "\n",
    "# 3) Calculer le max drawdown global\n",
    "max_dd_global_ndx = portfolio_per_day['S&P500_Drawdown'].min()\n",
    "\n",
    "# 4) Calculer le drawdown maximal par année\n",
    "#    On regroupe par année et on prend le minimum (le plus négatif) de Drawdown\n",
    "mensuel_max_dd_ndx = (\n",
    "    portfolio_per_day['S&P500_Drawdown']\n",
    "    .groupby(portfolio_per_day.index.year)\n",
    "    .min()\n",
    "    .rename('MaxDrawdown')\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "portfolio_per_month = portfolio_per_day.resample('ME').agg({\n",
    "    'Capital': 'last',\n",
    "    'Ticker_Count': 'mean',\n",
    "    'Drawdown': 'min',\n",
    "    'S&P500_capital': 'last',\n",
    "    'S&P500_Drawdown': 'min'\n",
    "})\n",
    "first_capital_month = portfolio_per_day['Capital'].resample('ME').first().ffill()\n",
    "last_capital_month = portfolio_per_day['Capital'].resample('ME').last().ffill()\n",
    "first_capital_ndx_month = portfolio_per_day['S&P500_capital'].resample('ME').first().ffill()\n",
    "last_capital_ndx_month = portfolio_per_day['S&P500_capital'].resample('ME').last().ffill()\n",
    "\n",
    "portfolio_per_month['Return'] = ((last_capital_month / first_capital_month - 1) * 100).fillna(0)\n",
    "portfolio_per_month['S&P500_return'] = ((last_capital_ndx_month / first_capital_ndx_month - 1) * 100).fillna(0)\n",
    "\n",
    "portfolio_per_annual = portfolio_per_day.resample('YE').agg({\n",
    "    'Capital': 'last',\n",
    "    'Ticker_Count': 'mean',\n",
    "    'Drawdown': 'min',\n",
    "    'S&P500_capital': 'last',\n",
    "    'S&P500_Drawdown': 'min'\n",
    "})\n",
    "first_capital_year = portfolio_per_day['Capital'].resample('YE').first().ffill()\n",
    "first_capital_ndx_year = portfolio_per_day['S&P500_capital'].resample('YE').first().ffill()\n",
    "last_capital_year = portfolio_per_day['Capital'].resample('YE').last().ffill()\n",
    "last_capital_ndx_year = portfolio_per_day['S&P500_capital'].resample('YE').first().ffill()\n",
    "\n",
    "portfolio_per_annual['Return'] = ((last_capital_year / first_capital_year - 1) * 100).fillna(0)\n",
    "portfolio_per_annual['S&P500_return'] = ((last_capital_ndx_year / first_capital_ndx_year - 1) * 100).fillna(0)"
   ],
   "id": "934ab6fe9aa830fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comparaison_benchmark = portfolio_per_month[portfolio_per_month['Return'] < portfolio_per_month['S&P500_return']].sort_values(by=['Date'], ascending=True)\n",
    "print(len(comparaison_benchmark)/len(portfolio_per_month))\n",
    "comparaison_benchmark.head(10)"
   ],
   "id": "deec7f84043c1516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisation",
   "id": "a093528cd0ea7c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CAGR_Portfolio = Calculate_CAGR(portfolio_per_annual['Capital'], len(portfolio_per_annual)).round(2)\n",
    "CAGR_ndx = Calculate_CAGR(portfolio_per_annual['S&P500_capital'], len(portfolio_per_annual)).round(2)\n",
    "print(f'CAGR_Portfolio:{CAGR_Portfolio}')\n",
    "print(f'CAGR_ndx:{CAGR_ndx}')"
   ],
   "id": "705e6e7e-2f01-4a13-bde8-b248a9af730e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Alpha_Portfolio = Calculate_Alpha_Jensen(portfolio_per_day['Return'], portfolio_per_day['S&P500_return'])\n",
    "print(f'Alpha_Portfolio:{Alpha_Portfolio}')"
   ],
   "id": "d7304d29e068b348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Sharpe_Portfolio = Calculate_Sharpe_Ratio(portfolio_per_month['Capital'], portfolio_per_month['Return']).round(2)\n",
    "Sharpe_ndx = Calculate_Sharpe_Ratio(portfolio_per_month['S&P500_capital'], portfolio_per_month['S&P500_return']).round(2)\n",
    "print(f'Sharpe_Portfolio:{Sharpe_Portfolio}')\n",
    "print(f'Sharpe_ndx:{Sharpe_ndx}')"
   ],
   "id": "fddbcbb9-d0c3-4e3c-8db3-b79ed66fb5ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Total_Returns_Portfolio = Calculate_Total_Returns(portfolio_per_month['Capital']).round(2)\n",
    "Total_Returns_ndx = Calculate_Total_Returns(portfolio_per_month['S&P500_capital']).round(2)\n",
    "print(f'Total_Return_Portfolio:{Total_Returns_Portfolio}')\n",
    "print(f'Total_Return_ndx:{Total_Returns_ndx}')"
   ],
   "id": "9d15a715-dd1f-44ef-b130-e2c1cb86d958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Max_Drawdown_Portfolio = Calculate_Max_Drawdown(portfolio_per_day['Return']).round(2)\n",
    "Max_Drawdown_ndx = Calculate_Max_Drawdown(portfolio_per_day['S&P500_return']).round(2)\n",
    "print(f'Max_Drawdown_Portfolio:{Max_Drawdown_Portfolio}')\n",
    "print(f'Max_Drawdown_ndx:{Max_Drawdown_ndx}')"
   ],
   "id": "1bca4f36-1bcb-4c08-a485-8d49bcf49339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Volatility_Portfolio = Calculate_Volatility(portfolio_per_month['Return']).round(2)\n",
    "Volatility_ndx = Calculate_Volatility(portfolio_per_month['S&P500_return']).round(2)\n",
    "print(f'Volatility_Portfolio:{Volatility_Portfolio}')\n",
    "print(f'Volatility_ndx:{Volatility_ndx}')"
   ],
   "id": "7b6289e7b9e3d19e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6) Tracer l’évolution du capital vs benchmark (échelle log)\n",
    "fig = px.line(\n",
    "    portfolio_per_month.reset_index(),\n",
    "    x=\"Date\",\n",
    "    y=[\"Capital\", \"S&P500_capital\"],\n",
    "    title=\"Évolution du capital vs S&P 500 (benchmark)\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "\n",
    "fig.update_yaxes(type=\"log\")  # axe Y en échelle logarithmique\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat=\"%Y-%m\",\n",
    "    xaxis_title=\"Mois\",\n",
    "    yaxis_title=\"Valeur (log)\",\n",
    "    legend_title=\"Courbes\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(\n",
    "    portfolio_per_month.reset_index(),\n",
    "    x=\"Date\",\n",
    "    y=[\"Return\", \"S&P500_return\"],\n",
    "    barmode=\"group\",\n",
    "    title=\"Return evolution vs S&P500 (benchmark)\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "# axe Y en échelle logarithmique\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat=\"%Y-%m\",\n",
    "    xaxis_title=\"Mois\",\n",
    "    yaxis_title=\"Valeur (log)\",\n",
    "    legend_title=\"Courbes\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.area(\n",
    "    portfolio_per_month.reset_index(),\n",
    "    x='Date',\n",
    "    y=[\"Drawdown\", \"S&P500_Drawdown\"],\n",
    "    title=\"Drawdown mensuel\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat='%Y-%m',\n",
    "    yaxis_tickformat='%.0%',\n",
    "    yaxis_title=\"Drawdown\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(\n",
    "    portfolio_per_month.reset_index(),\n",
    "    x=\"Date\",\n",
    "    y=\"Ticker_Count\",\n",
    "    title=\"Counts tickers by month\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "# axe Y en échelle logarithmique\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat=\"%Y-%m\",\n",
    "    xaxis_title=\"Mois\",\n",
    "    yaxis_title=\"Valeur (log)\",\n",
    "    legend_title=\"Courbes\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    portfolio_per_annual.reset_index(),\n",
    "    x=\"Date\",\n",
    "    y=[\"Capital\", \"S&P500_capital\"],\n",
    "    title=\"Évolution du capital vs S&P500 (benchmark)\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "\n",
    "fig.update_yaxes(type=\"log\")  # axe Y en échelle logarithmique\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat=\"%Y-%m\",\n",
    "    xaxis_title=\"Mois\",\n",
    "    yaxis_title=\"Valeur (log)\",\n",
    "    legend_title=\"Courbes\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(\n",
    "    portfolio_per_annual.reset_index(),\n",
    "    x=\"Date\",\n",
    "    y=[\"Return\", \"S&P500_return\"],\n",
    "    barmode=\"group\",\n",
    "    title=\"Return evolution vs S&P500 (benchmark)\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "# axe Y en échelle logarithmique\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat=\"%Y-%m\",\n",
    "    xaxis_title=\"Mois\",\n",
    "    yaxis_title=\"Valeur (log)\",\n",
    "    legend_title=\"Courbes\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.area(\n",
    "    portfolio_per_annual.reset_index(),\n",
    "    x='Date',\n",
    "    y=[\"Drawdown\", \"S&P500_Drawdown\"],\n",
    "    title=\"Drawdown annuel\",\n",
    "    labels={\"value\": \"Valeur\", \"variable\": \"Série\", \"Date\": \"Mois\"}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat='%Y-%m',\n",
    "    yaxis_tickformat='%.0%',\n",
    "    yaxis_title=\"Drawdown\",\n",
    ")\n",
    "fig.show()"
   ],
   "id": "49f9a1f818516981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "weekly_data[weekly_data['Ticker'] == 'PLTR'][['Date', 'Ticker', 'ma_9', 'ma_25', 'Close', 'Open']].tail(20)",
   "id": "dff9428105f20b74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "faddf5899de51314",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
